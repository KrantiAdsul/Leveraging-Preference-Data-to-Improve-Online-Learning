In this project, we wish to investigate the extent to which offline demonstration data can improve online learning. It is natural to expect some improvement, but the question is how, and by how much? We plan to characterize the quality of demonstration data to generate portable insights as we focus on Thompson sampling (TS) applied to a multiarmed bandit as a prototypical online learning algorithm and model. The demonstrationdata shall be generated by a (expert) rater with a given ‘competence’ level. Based on this theoretical algorithm, we plan to conduct experiments, if possible, or then develop a practical TS algorithm that utilizes the demonstration data in a coherent. This shall offer insight into how pretraining can greatly improve online performance as is seen currently in the Reinforcement Learning with Human Feedback (RLHF) paradigm, or in the recommender system space
